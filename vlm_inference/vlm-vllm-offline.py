import json
from PIL import Image
from transformers import AutoProcessor
from vllm import LLM, SamplingParams # vllm >= 0.7.3
from tqdm import tqdm

INPUT_JSONL = "gaia_valid_with_text_or_pic_tasks.jsonl"                   
OUTPUT_JSON = "Qwen2.5-VL-7B-Instruct_valid_text_or_pic_outputs.json"
MODEL_PATH = "/share_data/data2/models/Qwen/Qwen2.5-VL-7B-Instruct"

# 1ï¸âƒ£ Load processor + vLLM engine
processor = AutoProcessor.from_pretrained(
    MODEL_PATH, 
    min_pixels=256*28*28, 
    max_pixels=1024*28*28, 
    trust_remote_code=True
)

llm = LLM(
    model=MODEL_PATH,
    trust_remote_code=True,
    tensor_parallel_size=4,
    device="cuda",
    gpu_memory_utilization=0.75,       # é™ä½é¢„åˆ†é…æ¯”ä¾‹
    max_model_len=4096,               # ç¼©çŸ­ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¤§å¹…é™ä½ KV-cache éœ€æ±‚
    max_num_seqs=1,                   # é™åˆ¶åŒæ—¶å¤„ç†çš„åºåˆ—æ•°
    max_num_batched_tokens=2048,      # chunked prefill æ¨èå€¼
    enable_chunked_prefill=True,       # å‡å°‘å†…å­˜ç¢ç‰‡
    # kv_cache_dtype="fp8",  # å°†KVç¼“å­˜ç²¾åº¦é™ä¸ºFP8
    dtype="bfloat16",  # æ¨¡å‹è®¡ç®—ä½¿ç”¨BF16æ··åˆç²¾åº¦
    limit_mm_per_prompt={"image": 1, "video": 0}, # æ¯ä¸ª prompt æœ€å¤š 1 å¼ å›¾
)

sampling_params = SamplingParams(
    temperature=0.1,
    top_p=0.001,
    repetition_penalty=1.05,
    max_tokens=1024,
    stop_token_ids=[],
)

def build_requests(path):
    reqs = []
    with open(path, encoding="utf-8") as f:
        for line in f:
            obj = json.loads(line)
            fname = obj.get("file_name","").strip()
            question = obj.get("Question","").strip()
            if not question:
                continue

            content = [{"type": "text", "text": question}]
            if fname:
                content.insert(0, {"type": "image", "path": fname})

            conv = [{"role": "user", "content": content}]
            prompt = processor.apply_chat_template(conv, tokenize=False, add_generation_prompt=True)
            entry = {"prompt": prompt}
            if fname:
                entry["multi_modal_data"] = {"image": Image.open(fname).convert("RGB")}
            reqs.append((obj, entry))
    return reqs

requests = build_requests(INPUT_JSONL)
print(f"ğŸ” Total prompts: {len(requests)}") # OK

results = []
skipped = 0
for original, req in tqdm(requests, desc="Generating", unit="item"):
    try:
        output = llm.generate(req, sampling_params=sampling_params, use_tqdm=False)[0]
        results.append({
            "Question": original["Question"],
            "file_name": original["file_name"],
            "response": output.outputs[0].text.strip()
        })
    except ValueError as e:
        if "too long to fit into the model" in str(e):
            skipped += 1
            print(f"âš ï¸ Prompt too long, skipped: {original['Question'][:50]}...")
            continue
        else:
            raise

with open(OUTPUT_JSON, "w", encoding="utf-8") as out:
    json.dump(results, out, ensure_ascii=False, indent=2)

print(f"âœ… Done â€” saved {len(results)} entries to {OUTPUT_JSON}")
print(f"âš ï¸ Skipped {skipped} prompts due to length") 