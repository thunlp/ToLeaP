## ğŸ› ï¸ Evaluation Method

<font size="5">**RoTBench Evaluation**</font>  

$\qquad$ RoTBench adapts three metrics, **Tool Selection (TS)**, **Parameter identification (PI)** and **Content filling (CF)**, to evalute funciton calling. Related methods are described in RoTBench_eval.py. To evaluate RoTBench, input should follow format:

$\qquad$ **Tool Selection (TS)** represents whether agent can choose right function.
**Parameter identification (PI)** represents whether agent can fill right parameter name into function.
**Content filling (CF)** denotes whether agent can fill corrent content into corresponding parameters.

$\qquad$ Input format include two files, **test_file** and **prediction_file**, which test_file should follow share_gpt file(.json) and prediction file should follow generated_predictions file format(.jsonl).

$\qquad$ Run RoTBench Evaluation:
```
python src/scripts/RoTBench_eval.py --test_file PATH --answer_file PATH
```

<font size="5"> **APIBANK Evaluation**</font>  
 
$\qquad$ APIBANK adapts two metrics, **Accuarcy of Tool Selection** and **Rouge** to evaluate function calling.  
$\qquad$ Run APIBANK Evaluation:
```
python src/scripts/APIBANK_eval.py.py --test_file PATH --answer_file PATH
```
<font size="5">**Toollens**</font>  
$\qquad$ Toollen apdats three metrics to evaluate tool retrieval abilities, as same with tool calling abilities in this scenario:**Recall@K**, **NDCG@K** and **COMP@K**.

$\qquad$ **Recall@K** metric measures how many relevant items were successfully retrieved from the entire dataset.  
$\qquad$ **NDCG@K** Normalized Discounted Cumulative Gain@K (NDCG@K) metric measures the system's ability to sort items based on relevance.  
$\qquad$ **COMP@K** measure whether the top-ğ¾ retrieved tools form a complete set with respect to the ground-truth set.  
$\qquad$ Run Toollens Evaluation:
```
#è¿™ä¸ªToollensæ¯”è¾ƒå¤æ‚ ä¸ºäº†é¢„é˜²çœ‹ä¸æ‡‚ï¼Œæˆ‘åŠ äº†ä¸­æ–‡æ³¨é‡Šï¼Œtopkæ˜¯ä¸€ä¸ªelementä¸ºint typeçš„list, æ¥æˆªæ–­æ£€ç´¢åˆ°çš„kä¸ªå·¥å…·ï¼›Ground_truth æ˜¯ä¸€ä¸ªlist ä»£è¡¨label, å³æœ‰å“ªäº›å·¥å…·å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼›pred_i æ˜¯å¦å¤–ä¸€ä¸ªlist ä»£è¡¨æ¨¡å‹é¢„æµ‹å‡ºæ¥ç”¨å“ªäº›å·¥å…·æ¥è§£å†³queryã€‚è¿™ç©æ„åŸç†ææ˜ç™½äº†ï¼Œç°åœ¨è¿˜ç”¨ä¸äº†ï¼Œä¸»è¦å­˜åœ¨åŒæ—¶å¤šä¸ªå·¥å…·è°ƒç”¨ï¼Œæ— æµ‹è¯•æ ·ä¾‹ç­‰é—®é¢˜ã€‚
python src/scripts/Toollens_eval.py --topk List[Int] --ground_truth_u_i List --pred_i List
```


 **Teval Evaluation**  (need fix, haven't finished yet)
 ```
python bfcl_eval.py
 ```

 **Teval Evaluation**  
 ```
python teval_eval.py
 ```

 **ToolAlpaca Evaluation**   
 `$EVAL_FILE` is the path to the evaluation file, expected to be a .jsonl file generated by LLAMA-Factory.  
 `$DATA_FILE` is the path to the data file, default to be the real api evaluation file obtained by `data/toolalpaca.sh`.
```
sh toolalpaca_eval.sh $EVAL_FILE $DATA_FILE
```
**TaskBench Evaluation**  
`$RESULT_PATH` is the path to the evaluation file, expected to be a .jsonl file generated by LLAMA-Factory.  
`$SRC_DATA_PATH` is the path to the data file, default to be the real api evaluation file obtained by `data/taskbench.sh`.
```
sh taskbench_eval.sh $RESULT_PATH $SRC_DATA_PATH
```